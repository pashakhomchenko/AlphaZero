{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pashakhomchenko/AlphaZero/blob/master/AlphaZero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZncR3Zh4xhFe"
      },
      "source": [
        "this notebook is my follow along and exploration of this AlphaZero [tutorial](https://www.youtube.com/watch?v=wuSQpLinRB4). let's dive in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj4owU_Uxg_a"
      },
      "source": [
        "# Tic Tac Toe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPdMIbhwxwKP"
      },
      "source": [
        "## Game Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ziiKrVPUE50K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M3I5U7spykI1"
      },
      "outputs": [],
      "source": [
        "class TicTacToe:\n",
        "  def __init__(self):\n",
        "    self.row_count = 3\n",
        "    self.column_count = 3\n",
        "    self.action_size = self.row_count * self.column_count\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"TicTacToe\"\n",
        "\n",
        "  def get_initial_state(self):\n",
        "    return np.zeros([self.row_count, self.column_count])\n",
        "\n",
        "  def get_next_state(self, state, action, player):\n",
        "    # action == 0 means top left corner, action == 9 - bottom right\n",
        "    row = action // self.column_count\n",
        "    column = action % self.column_count\n",
        "    state[row, column] = player\n",
        "    return state\n",
        "\n",
        "  def get_valid_moves(self, state):\n",
        "    # state.reshape(-1) - flattens the array\n",
        "    return (state.reshape(-1) == 0).astype(np.uint8)\n",
        "\n",
        "  def check_win(self, state, action):\n",
        "    if action == None:\n",
        "      return False\n",
        "\n",
        "    row = action // self.column_count\n",
        "    column = action % self.column_count\n",
        "    player = state[row, column]\n",
        "    return (\n",
        "        np.sum(state[row, :]) == player * self.column_count\n",
        "        or\n",
        "        np.sum(state[:, column]) == player * self.row_count\n",
        "        or\n",
        "        np.sum(np.diag(state)) == player * self.row_count\n",
        "        or\n",
        "        # flipping the state to get the other diagonal\n",
        "        np.sum(np.diag(np.flip(state))) == player * self.row_count\n",
        "    )\n",
        "\n",
        "  def get_value_and_terminated(self, state, action):\n",
        "    if self.check_win(state, action):\n",
        "      # win, reward is 1\n",
        "      return 1, True\n",
        "    if np.sum(self.get_valid_moves(state)) == 0:\n",
        "      # draw, reward is 0\n",
        "      return 0, True\n",
        "    # continue the game\n",
        "    return 0, False\n",
        "\n",
        "  def get_opponent(self, player):\n",
        "    return -player\n",
        "\n",
        "  def get_opponent_value(self, value):\n",
        "    return -value\n",
        "\n",
        "  def change_perspective(self, state, player):\n",
        "    return state * player\n",
        "\n",
        "  def get_encoded_state(self, state):\n",
        "    encoded_state = np.stack(\n",
        "        (state == -1, state == 0, state == 1)\n",
        "    ).astype(np.float32)\n",
        "    return encoded_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AzY3PV106hr"
      },
      "source": [
        "## Monte Carlo Tree Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qUOMB2Q2z1-6"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "  def __init__(self, game, args, state, parent=None, action_taken=None):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.state = state\n",
        "    self.parent = parent\n",
        "    self.action_taken = action_taken\n",
        "\n",
        "    self.children = []\n",
        "    self.expandable_moves = game.get_valid_moves(state)\n",
        "\n",
        "    self.visit_count = 0\n",
        "    self.value_sum = 0\n",
        "\n",
        "  def is_fully_expanded(self):\n",
        "    # no moves and at least one child to select\n",
        "    # if there are no moves and children, no child can be created - all options explored, cannot expand, continue selection\n",
        "    # if there are no moves and no children, the game is over - is_terminated will be true, stop selection\n",
        "    # if there are moves then we found a leaf node and can expand it, stop selection\n",
        "    return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n",
        "\n",
        "  def select(self):\n",
        "    best_child = None\n",
        "    best_ucb = -np.inf\n",
        "\n",
        "    for child in self.children:\n",
        "      ucb = self.get_ucb(child)\n",
        "      if ucb > best_ucb:\n",
        "        best_child = child\n",
        "        best_ucb = ucb\n",
        "\n",
        "    return best_child\n",
        "\n",
        "  def get_ucb(self, child):\n",
        "    # rescale to [0,1] range\n",
        "    # Take inverse of q because the child is the opponent from perspective\n",
        "    # of the parent, so we are looking for the worst q\n",
        "    # (parent player 1, child player 2)\n",
        "    q_value = 1 - (child.value_sum / child.visit_count + 1 ) / 2\n",
        "    return q_value + self.args['C'] * math.sqrt(math.log(self.visit_count) / child.visit_count)\n",
        "\n",
        "  def expand(self):\n",
        "    # select random move\n",
        "    action = np.random.choice(np.where(self.expandable_moves == 1)[0])\n",
        "    # no more expandable\n",
        "    self.expandable_moves[action] = 0\n",
        "\n",
        "    # create the new state that child will take\n",
        "    child_state = self.state.copy()\n",
        "    child_state = self.game.get_next_state(child_state, action, 1)\n",
        "    child_state = self.game.change_perspective(child_state, -1)\n",
        "\n",
        "    # create child\n",
        "    child = Node(self.game, self.args, child_state, self, action)\n",
        "    self.children.append(child)\n",
        "    return child\n",
        "\n",
        "  def simulate(self):\n",
        "    value, is_terminated = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
        "    value = self.game.get_opponent_value(value)\n",
        "\n",
        "    if is_terminated:\n",
        "      return value\n",
        "\n",
        "    # random playing until the game finishes\n",
        "    rollout_state = self.state.copy()\n",
        "    rollout_player = 1\n",
        "    while True:\n",
        "      valid_moves = self.game.get_valid_moves(rollout_state)\n",
        "      action = np.random.choice(np.where(valid_moves == 1)[0])\n",
        "      rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n",
        "      value, is_terminated = self.game.get_value_and_terminated(rollout_state, action)\n",
        "      if is_terminated:\n",
        "        if rollout_player == -1:\n",
        "          value = self.game.get_opponent_value(value)\n",
        "        return value\n",
        "\n",
        "      rollout_player = self.game.get_opponent(rollout_player)\n",
        "\n",
        "  def backpropogate(self, value):\n",
        "    # update yourself\n",
        "    self.value_sum += value\n",
        "    self.visit_count += 1\n",
        "\n",
        "    # update parent\n",
        "    value = self.game.get_opponent_value(value)\n",
        "    if self.parent is not None:\n",
        "      self.parent.backpropogate(value)\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "  def __init__(self, game, args):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "\n",
        "  def search(self, state):\n",
        "    root = Node(self.game, self.args, state)\n",
        "\n",
        "    for search in range(self.args['num_searches']):\n",
        "      node = root\n",
        "\n",
        "      # Selection\n",
        "      while node.is_fully_expanded():\n",
        "        node = node.select()\n",
        "\n",
        "      value, is_terminated = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "      # value above is value of the opponent\n",
        "      value = self.game.get_opponent_value(value)\n",
        "\n",
        "      if not is_terminated:\n",
        "        # Expansion\n",
        "        node = node.expand()\n",
        "        # Simulation\n",
        "        value = node.simulate()\n",
        "\n",
        "      # Backpropogation\n",
        "      node.backpropogate(value)\n",
        "\n",
        "    action_probs = np.zeros(self.game.action_size)\n",
        "    for child in root.children:\n",
        "      action_probs[child.action_taken] = child.visit_count\n",
        "    action_probs /= np.sum(action_probs)\n",
        "    return action_probs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfp7b1UfNk_b"
      },
      "source": [
        "## Game with MCTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hO_7673Nx09o"
      },
      "outputs": [],
      "source": [
        "tictactoe = TicTacToe()\n",
        "player = 1\n",
        "\n",
        "args = {\n",
        "    'C': 1.41,\n",
        "    'num_searches': 1000\n",
        "}\n",
        "mcts = MCTS(tictactoe, args)\n",
        "\n",
        "state = tictactoe.get_initial_state()\n",
        "\n",
        "# while True:\n",
        "#   print(state)\n",
        "\n",
        "#   if player == 1:\n",
        "#     valid_moves = tictactoe.get_valid_moves(state)\n",
        "#     print(\"valid moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
        "#     action = int(input(f\"{player}:\"))\n",
        "\n",
        "#     if valid_moves[action] == 0:\n",
        "#       print(\"action not valid\")\n",
        "#       continue\n",
        "#   else:\n",
        "#     neutral_state = tictactoe.change_perspective(state, player)\n",
        "#     mcts_probs = mcts.search(neutral_state)\n",
        "#     action = np.argmax(mcts_probs)\n",
        "\n",
        "#   state = tictactoe.get_next_state(state, action, player)\n",
        "#   value, is_terminated = tictactoe.get_value_and_terminated(state, action)\n",
        "\n",
        "#   if is_terminated:\n",
        "#     print(state)\n",
        "#     if value == 1:\n",
        "#       print(player, \"won\")\n",
        "#     else:\n",
        "#       print(\"draw\")\n",
        "#     break\n",
        "\n",
        "#   # switch to the next player\n",
        "#   player = tictactoe.get_opponent(player)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ0Qv9mSXqak"
      },
      "source": [
        "too easy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXB5fRpWNwUc"
      },
      "source": [
        "##AlphaMCTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "INbygRHwN6_r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "from tqdm.notebook import trange\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eu1W3fzhN9hM"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.activation import Softmax\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, game, num_resBlocks, num_hidden, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.startBlock = nn.Sequential(\n",
        "        nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(num_hidden),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.backBone = nn.ModuleList(\n",
        "        [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
        "    )\n",
        "    self.policyHead = nn.Sequential(\n",
        "        nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(32 * game.row_count * game.column_count, game.action_size), # policy output\n",
        "        # why not add softmax in here??\n",
        "    )\n",
        "    self.valueHead = nn.Sequential(\n",
        "        nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(3),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3 * game.row_count * game.column_count, 1), # value output\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.startBlock(x)\n",
        "    for resBlock in self.backBone:\n",
        "      x = resBlock(x)\n",
        "    policy = self.policyHead(x)\n",
        "    value = self.valueHead(x)\n",
        "    return policy, value\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, num_hidden):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(num_hidden)\n",
        "    self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(num_hidden)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.bn2(self.conv2(x))\n",
        "    x += residual # skip connection, allows to mask out conv\n",
        "    x = F.relu(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dzivRYMLWGl0"
      },
      "outputs": [],
      "source": [
        "class AlphaNode:\n",
        "  def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.state = state\n",
        "    self.parent = parent\n",
        "    self.action_taken = action_taken\n",
        "    self.prior = prior # prob that is given by parent to this node\n",
        "\n",
        "    self.children = []\n",
        "\n",
        "    self.visit_count = visit_count\n",
        "    self.value_sum = 0\n",
        "\n",
        "  def is_fully_expanded(self):\n",
        "    # now we exapnd in all direction immediately as we have policy\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  def select(self):\n",
        "    best_child = None\n",
        "    best_ucb = -np.inf\n",
        "\n",
        "    for child in self.children:\n",
        "      ucb = self.get_ucb(child)\n",
        "      if ucb > best_ucb:\n",
        "        best_child = child\n",
        "        best_ucb = ucb\n",
        "\n",
        "    return best_child\n",
        "\n",
        "  def get_ucb(self, child):\n",
        "    # rescale to [0,1] range\n",
        "    # Take inverse of q because the child is the opponent from perspective\n",
        "    # of the parent, so we are looking for the worst q\n",
        "    # (parent player 1, child player 2)\n",
        "    if child.visit_count == 0:\n",
        "        q_value = 0\n",
        "    else:\n",
        "        q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
        "    return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
        "\n",
        "  def expand(self, policy):\n",
        "    for action, prob in enumerate(policy):\n",
        "      if prob > 0:\n",
        "        # create the new state that child will take\n",
        "        child_state = self.state.copy()\n",
        "        child_state = self.game.get_next_state(child_state, action, 1)\n",
        "        child_state = self.game.change_perspective(child_state, -1)\n",
        "\n",
        "        # create child\n",
        "        child = AlphaNode(self.game, self.args, child_state, self, action, prob)\n",
        "        self.children.append(child)\n",
        "\n",
        "\n",
        "  def backpropogate(self, value):\n",
        "    # update yourself\n",
        "    self.value_sum += value\n",
        "    self.visit_count += 1\n",
        "\n",
        "    # update parent\n",
        "    value = self.game.get_opponent_value(value)\n",
        "    if self.parent is not None:\n",
        "      self.parent.backpropogate(value)\n",
        "\n",
        "class AlphaMCTS:\n",
        "  def __init__(self, game, args, model):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.model = model\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def search(self, state):\n",
        "    # Add visit count for the first UCB calculation\n",
        "    root = AlphaNode(self.game, self.args, state, visit_count=1)\n",
        "\n",
        "    policy, _ = self.model(\n",
        "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
        "    )\n",
        "    policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Add dirichlet noise to the starting policy to explore more at the start\n",
        "    policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
        "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
        "\n",
        "    valid_moves = self.game.get_valid_moves(state)\n",
        "    policy *= valid_moves\n",
        "    policy /= np.sum(policy) # turn into probs\n",
        "    root.expand(policy)\n",
        "\n",
        "\n",
        "    for search in range(self.args['num_searches']):\n",
        "      node = root\n",
        "\n",
        "      # Selection\n",
        "      while node.is_fully_expanded():\n",
        "        node = node.select()\n",
        "\n",
        "      value, is_terminated = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "      # value above is value of the opponent\n",
        "      value = self.game.get_opponent_value(value)\n",
        "\n",
        "      if not is_terminated:\n",
        "        policy, value = self.model(\n",
        "            # batch dim, as we don't have batches, it's just a singleton dim\n",
        "            torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
        "        )\n",
        "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy() # get rid of batch dim\n",
        "\n",
        "        # mask out illiegal moves\n",
        "        valid_moves = self.game.get_valid_moves(node.state)\n",
        "        policy *= valid_moves\n",
        "        policy /= np.sum(policy)\n",
        "\n",
        "        value = value.item()\n",
        "\n",
        "        # Expansion\n",
        "        node.expand(policy)\n",
        "\n",
        "        # Simulation - no longer needed\n",
        "        # value = node.simulate()\n",
        "\n",
        "      # Backpropogation\n",
        "      node.backpropogate(value)\n",
        "\n",
        "    action_probs = np.zeros(self.game.action_size)\n",
        "    for child in root.children:\n",
        "      action_probs[child.action_taken] = child.visit_count\n",
        "    action_probs /= np.sum(action_probs)\n",
        "    return action_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XwNOcKKcZ3Ia"
      },
      "outputs": [],
      "source": [
        "tictactoe = TicTacToe()\n",
        "player = 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 1000\n",
        "}\n",
        "model = ResNet(tictactoe, 4, 64, device)\n",
        "model.eval()\n",
        "\n",
        "mcts = AlphaMCTS(tictactoe, args, model)\n",
        "\n",
        "state = tictactoe.get_initial_state()\n",
        "\n",
        "# while True:\n",
        "#   print(state)\n",
        "\n",
        "#   if player == 1:\n",
        "#     valid_moves = tictactoe.get_valid_moves(state)\n",
        "#     print(\"valid moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
        "#     action = int(input(f\"{player}:\"))\n",
        "\n",
        "#     if valid_moves[action] == 0:\n",
        "#       print(\"action not valid\")\n",
        "#       continue\n",
        "#   else:\n",
        "#     neutral_state = tictactoe.change_perspective(state, player)\n",
        "#     mcts_probs = mcts.search(neutral_state)\n",
        "#     action = np.argmax(mcts_probs)\n",
        "\n",
        "#   state = tictactoe.get_next_state(state, action, player)\n",
        "#   value, is_terminated = tictactoe.get_value_and_terminated(state, action)\n",
        "\n",
        "#   if is_terminated:\n",
        "#     print(state)\n",
        "#     if value == 1:\n",
        "#       print(player, \"won\")\n",
        "#     else:\n",
        "#       print(\"draw\")\n",
        "#     break\n",
        "\n",
        "#   # switch to the next player\n",
        "#   player = tictactoe.get_opponent(player)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAP9ZDypmXOF"
      },
      "source": [
        "##AlphaZero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8FU8DGOzmbMh"
      },
      "outputs": [],
      "source": [
        "class AlphaZero:\n",
        "  def __init__(self, model, optimizer, game, args):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.mcts = AlphaMCTS(game, args, model)\n",
        "\n",
        "  def selfPlay(self):\n",
        "    memory = [] # training data for the model for a single game\n",
        "    player = 1\n",
        "    state = self.game.get_initial_state()\n",
        "\n",
        "    while True:\n",
        "      neutral_state = self.game.change_perspective(state, player)\n",
        "      action_probs = self.mcts.search(neutral_state)\n",
        "\n",
        "      memory.append((neutral_state, action_probs, player))\n",
        "\n",
        "      # explore / exploit control\n",
        "      temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
        "      temperature_action_probs /= np.sum(temperature_action_probs)\n",
        "      action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
        "      state = self.game.get_next_state(state, action, player)\n",
        "\n",
        "      value, is_terminated = self.game.get_value_and_terminated(state, action)\n",
        "\n",
        "      if is_terminated:\n",
        "        returnMemory = []\n",
        "        for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
        "          hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "          returnMemory.append((self.game.get_encoded_state(hist_neutral_state),\n",
        "                           hist_action_probs,\n",
        "                           hist_outcome))\n",
        "        return returnMemory\n",
        "\n",
        "      player = self.game.get_opponent(player)\n",
        "\n",
        "  def train(self, memory):\n",
        "    random.shuffle(memory)\n",
        "    for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "      sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
        "      state, policy_targets, value_targets = zip(*sample)\n",
        "\n",
        "      state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
        "\n",
        "      state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
        "      policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
        "      value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
        "\n",
        "      out_policy, out_value = self.model(state)\n",
        "\n",
        "      # how is this the right loss, if policy_targets are also partially generated by the model\n",
        "      policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
        "      value_loss = F.mse_loss(out_value, value_targets)\n",
        "      loss = policy_loss + value_loss\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "  def learn(self):\n",
        "    for iteration in range(self.args['num_iterations']):\n",
        "      memory = [] # training data for the model\n",
        "\n",
        "      self.model.eval()\n",
        "      for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
        "        memory += self.selfPlay()\n",
        "\n",
        "      self.model.train()\n",
        "      for epoch in trange(self.args['num_epochs']):\n",
        "        self.train(memory)\n",
        "\n",
        "      torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "      torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395,
          "referenced_widgets": [
            "52e22e1c7bac46df8359860300860004",
            "115936d4a4454ad79ce51dc108577449",
            "bba63de401324fb2abd728059653b3e4",
            "9c8e30ab64864439802dc1845fc1884b",
            "d4f18c200301471f8788af5ec7725841",
            "1c12e96999cd46da8d428b2d0a90c13c",
            "082f6b708e164796b408fa7514f45714",
            "1b12e91f5fb944d4a94d5d3001ce5407",
            "0bf86046e36945d980c6aee488b0a39c",
            "e1c8939894834dfea5936a472b4602d3",
            "0a7f26ee57894f228a14c37a0485a09a"
          ]
        },
        "id": "fuWtnyYvqsL-",
        "outputId": "7a4751a0-5106-4040-e4e9-34e6275c8081"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52e22e1c7bac46df8359860300860004"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-290e10f74a49>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0malphaZero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlphaZero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtictactoe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0malphaZero\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-9eb5ec8f7279>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mselfPlay_iteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_selfPlay_iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselfPlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9eb5ec8f7279>\u001b[0m in \u001b[0;36mselfPlay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mneutral_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_perspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneutral_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneutral_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-09e4800247fc>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlphaNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisit_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     policy, _ = self.model(\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoded_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5e997dc1b1bd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mresBlock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackBone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "tictactoe = TicTacToe()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ResNet(tictactoe, 4, 64, device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 60,\n",
        "    'num_iterations': 3,\n",
        "    'num_selfPlay_iterations': 500,\n",
        "    'num_epochs': 4,\n",
        "    'batch_size': 64,\n",
        "    'temperature': 1.25, # explore / exploit control\n",
        "    'dirichlet_epsilon': 0.25, # noise params\n",
        "    'dirichlet_alpha': 0.3\n",
        "}\n",
        "\n",
        "alphaZero = AlphaZero(model, optimizer, tictactoe, args)\n",
        "alphaZero.learn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShcWjPTQ8d_w"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tictactoe = TicTacToe()\n",
        "\n",
        "state = tictactoe.get_initial_state()\n",
        "state = tictactoe.get_next_state(state, 2, -1)\n",
        "state = tictactoe.get_next_state(state, 4, -1)\n",
        "state = tictactoe.get_next_state(state, 6, 1)\n",
        "state = tictactoe.get_next_state(state, 8, 1)\n",
        "\n",
        "encoded_state = tictactoe.get_encoded_state(state) # 3 planes\n",
        "\n",
        "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
        "\n",
        "model = ResNet(tictactoe, 4, 64, device)\n",
        "model.load_state_dict(torch.load(\"model_2.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "policy, value = model(tensor_state)\n",
        "value = value.item()\n",
        "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "print(value)\n",
        "print(state)\n",
        "\n",
        "plt.bar(range(tictactoe.action_size), policy)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Connect Four"
      ],
      "metadata": {
        "id": "KGljwK0eiBNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regular AlphaZero"
      ],
      "metadata": {
        "id": "j3gfa0PrpTsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConnectFour:\n",
        "  def __init__(self):\n",
        "    self.row_count = 6\n",
        "    self.column_count = 7\n",
        "    self.action_size = self.column_count\n",
        "    self.in_a_row = 4\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"ConnectFour\"\n",
        "\n",
        "  def get_initial_state(self):\n",
        "    return np.zeros([self.row_count, self.column_count])\n",
        "\n",
        "  def get_next_state(self, state, action, player):\n",
        "    # action is a column that we are playing\n",
        "    row = np.max(np.where(state[:, action] == 0))\n",
        "    state[row, action] = player\n",
        "    return state\n",
        "\n",
        "  def get_valid_moves(self, state):\n",
        "    # just check the top row\n",
        "    return (state[0] == 0).astype(np.uint8)\n",
        "\n",
        "  def check_win(self, state, action):\n",
        "    if action == None:\n",
        "      return False\n",
        "\n",
        "    row = np.max(np.where(state[:, action] != 0))\n",
        "    column = action\n",
        "    player = state[row, column]\n",
        "\n",
        "    def count(offset_row, offset_column):\n",
        "      for i in range(1, self.in_a_row):\n",
        "        r = row + offset_row * i\n",
        "        c = action + offset_column * i\n",
        "        if (\n",
        "            r < 0\n",
        "            or r >= self.row_count\n",
        "            or c < 0\n",
        "            or c >= self.column_count\n",
        "            or state[r, c] != player\n",
        "        ):\n",
        "          return i - 1\n",
        "      return self.in_a_row - 1\n",
        "\n",
        "    return (\n",
        "        count(1, 0) >= self.in_a_row - 1 # vertical\n",
        "        or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
        "        or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
        "        or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
        "    )\n",
        "\n",
        "  def get_value_and_terminated(self, state, action):\n",
        "    if self.check_win(state, action):\n",
        "      # win, reward is 1\n",
        "      return 1, True\n",
        "    if np.sum(self.get_valid_moves(state)) == 0:\n",
        "      # draw, reward is 0\n",
        "      return 0, True\n",
        "    # continue the game\n",
        "    return 0, False\n",
        "\n",
        "  def get_opponent(self, player):\n",
        "    return -player\n",
        "\n",
        "  def get_opponent_value(self, value):\n",
        "    return -value\n",
        "\n",
        "  def change_perspective(self, state, player):\n",
        "    return state * player\n",
        "\n",
        "  def get_encoded_state(self, state):\n",
        "    encoded_state = np.stack(\n",
        "        (state == -1, state == 0, state == 1)\n",
        "    ).astype(np.float32)\n",
        "    return encoded_state"
      ],
      "metadata": {
        "id": "RqXsT_ShidFi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is way too slow with a larger neural net, so we will need to parallelize"
      ],
      "metadata": {
        "id": "qVnSpVfRpjUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "game = ConnectFour()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ResNet(game, 9, 128, device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 600,\n",
        "    'num_iterations': 8,\n",
        "    'num_selfPlay_iterations': 500,\n",
        "    'num_epochs': 4,\n",
        "    'batch_size': 128,\n",
        "    'temperature': 1.25, # explore / exploit control\n",
        "    'dirichlet_epsilon': 0.25, # noise params\n",
        "    'dirichlet_alpha': 0.3\n",
        "}\n",
        "\n",
        "alphaZero = AlphaZero(model, optimizer, game, args)\n",
        "alphaZero.learn()"
      ],
      "metadata": {
        "id": "PHRkaxOHnSDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game = ConnectFour()\n",
        "player = 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 20,\n",
        "    'dirichlet_epsilon': 0.0, # noise params\n",
        "    'dirichlet_alpha': 0.3\n",
        "}\n",
        "model = ResNet(game, 9, 128, device)\n",
        "model.eval()\n",
        "\n",
        "mcts = AlphaMCTS(game, args, model)\n",
        "\n",
        "state = game.get_initial_state()\n",
        "\n",
        "while True:\n",
        "  print(state)\n",
        "\n",
        "  if player == 1:\n",
        "    valid_moves = game.get_valid_moves(state)\n",
        "    print(\"valid moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
        "    action = int(input(f\"{player}:\"))\n",
        "\n",
        "    if valid_moves[action] == 0:\n",
        "      print(\"action not valid\")\n",
        "      continue\n",
        "  else:\n",
        "    neutral_state = game.change_perspective(state, player)\n",
        "    mcts_probs = mcts.search(neutral_state)\n",
        "    action = np.argmax(mcts_probs)\n",
        "\n",
        "  state = game.get_next_state(state, action, player)\n",
        "  value, is_terminated = game.get_value_and_terminated(state, action)\n",
        "\n",
        "  if is_terminated:\n",
        "    print(state)\n",
        "    if value == 1:\n",
        "      print(player, \"won\")\n",
        "    else:\n",
        "      print(\"draw\")\n",
        "    break\n",
        "\n",
        "  # switch to the next player\n",
        "  player = game.get_opponent(player)"
      ],
      "metadata": {
        "id": "3HTRy6-emTub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel AlphaZero"
      ],
      "metadata": {
        "id": "FjkWYCkvpZdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaMCTSParellel:\n",
        "  def __init__(self, game, args, model):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.model = model\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def search(self, state):\n",
        "    # Add visit count for the first UCB calculation\n",
        "    root = AlphaNode(self.game, self.args, state, visit_count=1)\n",
        "\n",
        "    policy, _ = self.model(\n",
        "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
        "    )\n",
        "    policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Add dirichlet noise to the starting policy to explore more at the start\n",
        "    policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
        "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
        "\n",
        "    valid_moves = self.game.get_valid_moves(state)\n",
        "    policy *= valid_moves\n",
        "    policy /= np.sum(policy) # turn into probs\n",
        "    root.expand(policy)\n",
        "\n",
        "\n",
        "    for search in range(self.args['num_searches']):\n",
        "      node = root\n",
        "\n",
        "      # Selection\n",
        "      while node.is_fully_expanded():\n",
        "        node = node.select()\n",
        "\n",
        "      value, is_terminated = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "      # value above is value of the opponent\n",
        "      value = self.game.get_opponent_value(value)\n",
        "\n",
        "      if not is_terminated:\n",
        "        policy, value = self.model(\n",
        "            # batch dim, as we don't have batches, it's just a singleton dim\n",
        "            torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
        "        )\n",
        "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy() # get rid of batch dim\n",
        "\n",
        "        # mask out illiegal moves\n",
        "        valid_moves = self.game.get_valid_moves(node.state)\n",
        "        policy *= valid_moves\n",
        "        policy /= np.sum(policy)\n",
        "\n",
        "        value = value.item()\n",
        "\n",
        "        # Expansion\n",
        "        node.expand(policy)\n",
        "\n",
        "        # Simulation - no longer needed\n",
        "        # value = node.simulate()\n",
        "\n",
        "      # Backpropogation\n",
        "      node.backpropogate(value)\n",
        "\n",
        "    action_probs = np.zeros(self.game.action_size)\n",
        "    for child in root.children:\n",
        "      action_probs[child.action_taken] = child.visit_count\n",
        "    action_probs /= np.sum(action_probs)\n",
        "    return action_probs"
      ],
      "metadata": {
        "id": "wRTP0oYUp-7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaZeroParellel:\n",
        "  def __init__(self, model, optimizer, game, args):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.mcts = AlphaMCTSParellel(game, args, model)\n",
        "\n",
        "  def selfPlay(self):\n",
        "    return_memory = [] # training data for the model for a single game\n",
        "    player = 1\n",
        "    spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
        "\n",
        "    while len(spGames) > 0:\n",
        "      states = np.stack([spg.state for spg in spGames])\n",
        "\n",
        "      neutral_states = self.game.change_perspective(state, player)\n",
        "      action_probs = self.mcts.search(neutral_states)\n",
        "\n",
        "      memory.append((neutral_state, action_probs, player))\n",
        "\n",
        "      # explore / exploit control\n",
        "      temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
        "      temperature_action_probs /= np.sum(temperature_action_probs)\n",
        "      action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
        "      state = self.game.get_next_state(state, action, player)\n",
        "\n",
        "      value, is_terminated = self.game.get_value_and_terminated(state, action)\n",
        "\n",
        "      if is_terminated:\n",
        "        returnMemory = []\n",
        "        for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
        "          hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "          returnMemory.append((self.game.get_encoded_state(hist_neutral_state),\n",
        "                           hist_action_probs,\n",
        "                           hist_outcome))\n",
        "        return returnMemory\n",
        "\n",
        "      player = self.game.get_opponent(player)\n",
        "\n",
        "  def train(self, memory):\n",
        "    random.shuffle(memory)\n",
        "    for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "      sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
        "      state, policy_targets, value_targets = zip(*sample)\n",
        "\n",
        "      state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
        "\n",
        "      state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
        "      policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
        "      value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
        "\n",
        "      out_policy, out_value = self.model(state)\n",
        "\n",
        "      # how is this the right loss, if policy_targets are also partially generated by the model\n",
        "      policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
        "      value_loss = F.mse_loss(out_value, value_targets)\n",
        "      loss = policy_loss + value_loss\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "  def learn(self):\n",
        "    for iteration in range(self.args['num_iterations']):\n",
        "      memory = [] # training data for the model\n",
        "\n",
        "      self.model.eval()\n",
        "      for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parellel_games']):\n",
        "        memory += self.selfPlay()\n",
        "\n",
        "      self.model.train()\n",
        "      for epoch in trange(self.args['num_epochs']):\n",
        "        self.train(memory)\n",
        "\n",
        "      torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "      torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
      ],
      "metadata": {
        "id": "T-hHcarSpxb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SPG: # Self Play Game\n",
        "  def __init__(self, game):\n",
        "    self.state = game.get_initial_state()\n",
        "    self.memory = []\n",
        "    self.root = None\n",
        "    self.node = None"
      ],
      "metadata": {
        "id": "1LVwX-tRqMZG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjHUunkhh47eirjf0WRNa9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52e22e1c7bac46df8359860300860004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_115936d4a4454ad79ce51dc108577449",
              "IPY_MODEL_bba63de401324fb2abd728059653b3e4",
              "IPY_MODEL_9c8e30ab64864439802dc1845fc1884b"
            ],
            "layout": "IPY_MODEL_d4f18c200301471f8788af5ec7725841"
          }
        },
        "115936d4a4454ad79ce51dc108577449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c12e96999cd46da8d428b2d0a90c13c",
            "placeholder": "​",
            "style": "IPY_MODEL_082f6b708e164796b408fa7514f45714",
            "value": "  0%"
          }
        },
        "bba63de401324fb2abd728059653b3e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b12e91f5fb944d4a94d5d3001ce5407",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bf86046e36945d980c6aee488b0a39c",
            "value": 0
          }
        },
        "9c8e30ab64864439802dc1845fc1884b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c8939894834dfea5936a472b4602d3",
            "placeholder": "​",
            "style": "IPY_MODEL_0a7f26ee57894f228a14c37a0485a09a",
            "value": " 0/500 [00:06&lt;?, ?it/s]"
          }
        },
        "d4f18c200301471f8788af5ec7725841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c12e96999cd46da8d428b2d0a90c13c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "082f6b708e164796b408fa7514f45714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b12e91f5fb944d4a94d5d3001ce5407": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bf86046e36945d980c6aee488b0a39c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1c8939894834dfea5936a472b4602d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a7f26ee57894f228a14c37a0485a09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}