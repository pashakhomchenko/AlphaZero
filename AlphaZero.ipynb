{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYG7utsq/aP4pPtvj8N2F5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pashakhomchenko/AlphaZero/blob/master/AlphaZero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this notebook is my follow along and exploration of this AlphaZero [tutorial](https://www.youtube.com/watch?v=wuSQpLinRB4). let's dive in"
      ],
      "metadata": {
        "id": "ZncR3Zh4xhFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tic Tac Toe\n"
      ],
      "metadata": {
        "id": "Zj4owU_Uxg_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Game Setup"
      ],
      "metadata": {
        "id": "YPdMIbhwxwKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ziiKrVPUE50K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "  def __init__(self):\n",
        "    self.row_count = 3\n",
        "    self.column_count = 3\n",
        "    self.action_size = self.row_count * self.column_count\n",
        "\n",
        "  def get_initial_state(self):\n",
        "    return np.zeros([self.row_count, self.column_count])\n",
        "\n",
        "  def get_next_state(self, state, action, player):\n",
        "    # action == 0 means top left corner, action == 9 - bottom right\n",
        "    row = action // self.column_count\n",
        "    column = action % self.column_count\n",
        "    state[row, column] = player\n",
        "    return state\n",
        "\n",
        "  def get_valid_moves(self, state):\n",
        "    # state.reshape(-1) - flattens the array\n",
        "    return (state.reshape(-1) == 0).astype(np.uint8)\n",
        "\n",
        "  def check_win(self, state, action):\n",
        "    if action == None:\n",
        "      return False\n",
        "\n",
        "    row = action // self.column_count\n",
        "    column = action % self.column_count\n",
        "    player = state[row, column]\n",
        "    return (\n",
        "        np.sum(state[row, :]) == player * self.column_count\n",
        "        or\n",
        "        np.sum(state[:, column]) == player * self.row_count\n",
        "        or\n",
        "        np.sum(np.diag(state)) == player * self.row_count\n",
        "        or\n",
        "        # flipping the state to get the other diagonal\n",
        "        np.sum(np.diag(np.flip(state))) == player * self.row_count\n",
        "    )\n",
        "\n",
        "  def get_value_and_terminated(self, state, action):\n",
        "    if self.check_win(state, action):\n",
        "      # win, reward is 1\n",
        "      return 1, True\n",
        "    if np.sum(self.get_valid_moves(state)) == 0:\n",
        "      # draw, reward is 0\n",
        "      return 0, True\n",
        "    # continue the game\n",
        "    return 0, False\n",
        "\n",
        "  def get_opponent(self, player):\n",
        "    return -player\n",
        "\n",
        "  def get_opponent_value(self, value):\n",
        "    return -value\n",
        "\n",
        "  def change_perspective(self, state, player):\n",
        "    return state * player\n",
        "\n",
        "  def get_encoded_state(self, state):\n",
        "    encoded_state = np.stack(\n",
        "        (state == -1, state == 0, state == 1)\n",
        "    ).astype(np.float32)\n",
        "    return encoded_state"
      ],
      "metadata": {
        "id": "M3I5U7spykI1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo Tree Search"
      ],
      "metadata": {
        "id": "8AzY3PV106hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "  def __init__(self, game, args, state, parent=None, action_taken=None):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.state = state\n",
        "    self.parent = parent\n",
        "    self.action_taken = action_taken\n",
        "\n",
        "    self.children = []\n",
        "    self.expandable_moves = game.get_valid_moves(state)\n",
        "\n",
        "    self.visit_count = 0\n",
        "    self.value_sum = 0\n",
        "\n",
        "  def is_fully_expanded(self):\n",
        "    # no moves and at least one child to select\n",
        "    # if there are no moves and children, no child can be created - all options explored, cannot expand, continue selection\n",
        "    # if there are no moves and no children, the game is over - is_terminated will be true, stop selection\n",
        "    # if there are moves then we found a leaf node and can expand it, stop selection\n",
        "    return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n",
        "\n",
        "  def select(self):\n",
        "    best_child = None\n",
        "    best_ucb = -np.inf\n",
        "\n",
        "    for child in self.children:\n",
        "      ucb = self.get_ucb(child)\n",
        "      if ucb > best_ucb:\n",
        "        best_child = child\n",
        "        best_ucb = ucb\n",
        "\n",
        "    return best_child\n",
        "\n",
        "  def get_ucb(self, child):\n",
        "    # rescale to [0,1] range\n",
        "    # Take inverse of q because the child is the opponent from perspective\n",
        "    # of the parent, so we are looking for the worst q\n",
        "    # (parent player 1, child player 2)\n",
        "    q_value = 1 - (child.value_sum / child.visit_count + 1 ) / 2\n",
        "    return q_value + self.args['C'] * math.sqrt(math.log(self.visit_count) / child.visit_count)\n",
        "\n",
        "  def expand(self):\n",
        "    # select random move\n",
        "    action = np.random.choice(np.where(self.expandable_moves == 1)[0])\n",
        "    # no more expandable\n",
        "    self.expandable_moves[action] = 0\n",
        "\n",
        "    # create the new state that child will take\n",
        "    child_state = self.state.copy()\n",
        "    child_state = self.game.get_next_state(child_state, action, 1)\n",
        "    child_state = self.game.change_perspective(child_state, -1)\n",
        "\n",
        "    # create child\n",
        "    child = Node(self.game, self.args, child_state, self, action)\n",
        "    self.children.append(child)\n",
        "    return child\n",
        "\n",
        "  def simulate(self):\n",
        "    value, is_terminated = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
        "    value = self.game.get_opponent_value(value)\n",
        "\n",
        "    if is_terminated:\n",
        "      return value\n",
        "\n",
        "    # random playing until the game finishes\n",
        "    rollout_state = self.state.copy()\n",
        "    rollout_player = 1\n",
        "    while True:\n",
        "      valid_moves = self.game.get_valid_moves(rollout_state)\n",
        "      action = np.random.choice(np.where(valid_moves == 1)[0])\n",
        "      rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n",
        "      value, is_terminated = self.game.get_value_and_terminated(rollout_state, action)\n",
        "      if is_terminated:\n",
        "        if rollout_player == -1:\n",
        "          value = self.game.get_opponent_value(value)\n",
        "        return value\n",
        "\n",
        "      rollout_player = self.game.get_opponent(rollout_player)\n",
        "\n",
        "  def backpropogate(self, value):\n",
        "    # update yourself\n",
        "    self.value_sum += value\n",
        "    self.visit_count += 1\n",
        "\n",
        "    # update parent\n",
        "    value = self.game.get_opponent_value(value)\n",
        "    if self.parent is not None:\n",
        "      self.parent.backpropogate(value)\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "  def __init__(self, game, args):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "\n",
        "  def search(self, state):\n",
        "    root = Node(self.game, self.args, state)\n",
        "\n",
        "    for search in range(self.args['num_searches']):\n",
        "      node = root\n",
        "\n",
        "      # Selection\n",
        "      while node.is_fully_expanded():\n",
        "        node = node.select()\n",
        "\n",
        "      value, is_terminated = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "      # value above is value of the opponent\n",
        "      value = self.game.get_opponent_value(value)\n",
        "\n",
        "      if not is_terminated:\n",
        "        # Expansion\n",
        "        node = node.expand()\n",
        "        # Simulation\n",
        "        value = node.simulate()\n",
        "\n",
        "      # Backpropogation\n",
        "      node.backpropogate(value)\n",
        "\n",
        "    action_probs = np.zeros(self.game.action_size)\n",
        "    for child in root.children:\n",
        "      action_probs[child.action_taken] = child.visit_count\n",
        "    action_probs /= np.sum(action_probs)\n",
        "    return action_probs\n",
        "\n"
      ],
      "metadata": {
        "id": "qUOMB2Q2z1-6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Game with MCTS"
      ],
      "metadata": {
        "id": "sfp7b1UfNk_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tictactoe = TicTacToe()\n",
        "player = 1\n",
        "\n",
        "args = {\n",
        "    'C': 1.41,\n",
        "    'num_searches': 1000\n",
        "}\n",
        "mcts = MCTS(tictactoe, args)\n",
        "\n",
        "state = tictactoe.get_initial_state()\n",
        "\n",
        "while True:\n",
        "  print(state)\n",
        "\n",
        "  if player == 1:\n",
        "    valid_moves = tictactoe.get_valid_moves(state)\n",
        "    print(\"valid moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
        "    action = int(input(f\"{player}:\"))\n",
        "\n",
        "    if valid_moves[action] == 0:\n",
        "      print(\"action not valid\")\n",
        "      continue\n",
        "  else:\n",
        "    neutral_state = tictactoe.change_perspective(state, player)\n",
        "    mcts_probs = mcts.search(neutral_state)\n",
        "    action = np.argmax(mcts_probs)\n",
        "\n",
        "  state = tictactoe.get_next_state(state, action, player)\n",
        "  value, is_terminated = tictactoe.get_value_and_terminated(state, action)\n",
        "\n",
        "  if is_terminated:\n",
        "    print(state)\n",
        "    if value == 1:\n",
        "      print(player, \"won\")\n",
        "    else:\n",
        "      print(\"draw\")\n",
        "    break\n",
        "\n",
        "  # switch to the next player\n",
        "  player = tictactoe.get_opponent(player)"
      ],
      "metadata": {
        "id": "hO_7673Nx09o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "too easy"
      ],
      "metadata": {
        "id": "zZ0Qv9mSXqak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AlphaMCTS"
      ],
      "metadata": {
        "id": "AXB5fRpWNwUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "INbygRHwN6_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc31aed-f8c3-4433-c2b3-5d520234314d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x780b11765850>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import Softmax\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, game, num_resBlocks, num_hidden):\n",
        "    super().__init__()\n",
        "    self.startBlock = nn.Sequential(\n",
        "        nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(num_hidden),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.backBone = nn.ModuleList(\n",
        "        [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
        "    )\n",
        "    self.policyHead = nn.Sequential(\n",
        "        nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(32 * game.row_count * game.column_count, game.action_size), # policy output\n",
        "        # why not add softmax in here??\n",
        "    )\n",
        "    self.valueHead = nn.Sequential(\n",
        "        nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(3),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3 * game.row_count * game.column_count, 1), # value output\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.startBlock(x)\n",
        "    for resBlock in self.backBone:\n",
        "      x = resBlock(x)\n",
        "    policy = self.policyHead(x)\n",
        "    value = self.valueHead(x)\n",
        "    return policy, value\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, num_hidden):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(num_hidden)\n",
        "    self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(num_hidden)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.bn2(self.conv2(x))\n",
        "    x += residual # skip connection, allows to mask out conv\n",
        "    x = F.relu(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "eu1W3fzhN9hM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tictactoe = TicTacToe()\n",
        "\n",
        "state = tictactoe.get_initial_state()\n",
        "state = tictactoe.get_next_state(state, 2, 1)\n",
        "state = tictactoe.get_next_state(state, 7, -1)\n",
        "\n",
        "encoded_state = tictactoe.get_encoded_state(state) # 3 planes\n",
        "\n",
        "tensor_state = torch.tensor(encoded_state).unsqueeze(0)\n",
        "\n",
        "model = ResNet(tictactoe, 4, 64)\n",
        "\n",
        "policy, value = model(tensor_state)\n",
        "value = value.item()\n",
        "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "plt.bar(range(tictactoe.action_size), policy)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "svg1PlgSTUMg",
        "outputId": "aff136cc-6e48-4371-be4f-827db7447611"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaNode:\n",
        "  def __init__(self, game, args, state, parent=None, action_taken=None, prior=0):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.state = state\n",
        "    self.parent = parent\n",
        "    self.action_taken = action_taken\n",
        "    self.prior = prior # prob that is given by parent to this node\n",
        "\n",
        "    self.children = []\n",
        "\n",
        "    self.visit_count = 0\n",
        "    self.value_sum = 0\n",
        "\n",
        "  def is_fully_expanded(self):\n",
        "    # now we exapnd in all direction immediately as we have policy\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  def select(self):\n",
        "    best_child = None\n",
        "    best_ucb = -np.inf\n",
        "\n",
        "    for child in self.children:\n",
        "      ucb = self.get_ucb(child)\n",
        "      if ucb > best_ucb:\n",
        "        best_child = child\n",
        "        best_ucb = ucb\n",
        "\n",
        "    return best_child\n",
        "\n",
        "  def get_ucb(self, child):\n",
        "    # rescale to [0,1] range\n",
        "    # Take inverse of q because the child is the opponent from perspective\n",
        "    # of the parent, so we are looking for the worst q\n",
        "    # (parent player 1, child player 2)\n",
        "    if child.visit_count == 0:\n",
        "      q_value = 0\n",
        "    else:\n",
        "      q_value = 1 - (child.value_sum / child.visit_count + 1 ) / 2\n",
        "    return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
        "\n",
        "  def expand(self, policy):\n",
        "    for action, prob in enumerate(policy):\n",
        "      if prob > 0:\n",
        "        # create the new state that child will take\n",
        "        child_state = self.state.copy()\n",
        "        child_state = self.game.get_next_state(child_state, action, 1)\n",
        "        child_state = self.game.change_perspective(child_state, -1)\n",
        "\n",
        "        # create child\n",
        "        child = AlphaNode(self.game, self.args, child_state, self, action, prob)\n",
        "        self.children.append(child)\n",
        "\n",
        "  def backpropogate(self, value):\n",
        "    # update yourself\n",
        "    self.value_sum += value\n",
        "    self.visit_count += 1\n",
        "\n",
        "    # update parent\n",
        "    value = self.game.get_opponent_value(value)\n",
        "    if self.parent is not None:\n",
        "      self.parent.backpropogate(value)\n",
        "\n",
        "class AlphaMCTS:\n",
        "  def __init__(self, game, args, model):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.model = model\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def search(self, state):\n",
        "    root = AlphaNode(self.game, self.args, state)\n",
        "\n",
        "    for search in range(self.args['num_searches']):\n",
        "      node = root\n",
        "\n",
        "      # Selection\n",
        "      while node.is_fully_expanded():\n",
        "        node = node.select()\n",
        "\n",
        "      value, is_terminated = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "      # value above is value of the opponent\n",
        "      value = self.game.get_opponent_value(value)\n",
        "\n",
        "      if not is_terminated:\n",
        "        policy, value = self.model(\n",
        "            # batch dim, as we don't have batches, it's just a singleton dim\n",
        "            torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0)\n",
        "        )\n",
        "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy() # get rid of batch dim\n",
        "\n",
        "        # mask out illiegal moves\n",
        "        valid_moves = self.game.get_valid_moves(node.state)\n",
        "        policy *= valid_moves\n",
        "        policy /= np.sum(policy)\n",
        "\n",
        "        value = value.item()\n",
        "\n",
        "        # Expansion\n",
        "        node.expand(policy)\n",
        "\n",
        "        # Simulation - no longer needed\n",
        "        # value = node.simulate()\n",
        "\n",
        "      # Backpropogation\n",
        "      node.backpropogate(value)\n",
        "\n",
        "    action_probs = np.zeros(self.game.action_size)\n",
        "    for child in root.children:\n",
        "      action_probs[child.action_taken] = child.visit_count\n",
        "    action_probs /= np.sum(action_probs)\n",
        "    return action_probs"
      ],
      "metadata": {
        "id": "dzivRYMLWGl0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tictactoe = TicTacToe()\n",
        "player = 1\n",
        "\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 1000\n",
        "}\n",
        "model = ResNet(tictactoe, 4, 64)\n",
        "model.eval()\n",
        "\n",
        "mcts = AlphaMCTS(tictactoe, args, model)\n",
        "\n",
        "state = tictactoe.get_initial_state()\n",
        "\n",
        "while True:\n",
        "  print(state)\n",
        "\n",
        "  if player == 1:\n",
        "    valid_moves = tictactoe.get_valid_moves(state)\n",
        "    print(\"valid moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
        "    action = int(input(f\"{player}:\"))\n",
        "\n",
        "    if valid_moves[action] == 0:\n",
        "      print(\"action not valid\")\n",
        "      continue\n",
        "  else:\n",
        "    neutral_state = tictactoe.change_perspective(state, player)\n",
        "    mcts_probs = mcts.search(neutral_state)\n",
        "    action = np.argmax(mcts_probs)\n",
        "\n",
        "  state = tictactoe.get_next_state(state, action, player)\n",
        "  value, is_terminated = tictactoe.get_value_and_terminated(state, action)\n",
        "\n",
        "  if is_terminated:\n",
        "    print(state)\n",
        "    if value == 1:\n",
        "      print(player, \"won\")\n",
        "    else:\n",
        "      print(\"draw\")\n",
        "    break\n",
        "\n",
        "  # switch to the next player\n",
        "  player = tictactoe.get_opponent(player)"
      ],
      "metadata": {
        "id": "XwNOcKKcZ3Ia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}